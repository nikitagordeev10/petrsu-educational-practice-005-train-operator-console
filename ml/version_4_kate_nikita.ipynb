{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Введение и установка зависимостей"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wave\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import jiwer\n",
    "from pydub import AudioSegment\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:08.223902800Z",
     "start_time": "2024-10-12T15:00:07.812347300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Конфигурация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Конфигурация\n",
    "CONFIG = {\n",
    "    'data_dir': '../data/train/',\n",
    "    'annotation_dir': '../data/train/annotation/',\n",
    "    'val_dir': '../data/val/luga/',\n",
    "    'model_path': \"../model/vosk_model\",\n",
    "    'batch_size': 2,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'hidden_dim': 55,\n",
    "    'output_dim': 22,\n",
    "    'input_dim': 2  # Изменится в зависимости от векторизации\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:30.740612Z",
     "start_time": "2024-10-12T15:00:30.735597200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция для загрузки аннотаций\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def load_annotations(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Пример загрузки\n",
    "train_annotations = load_annotations(f\"{CONFIG['annotation_dir']}/hr_bot_synt.json\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:08.344396900Z",
     "start_time": "2024-10-12T15:00:07.852443100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создание модели и классификатора"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Создание экземпляра модели\n",
    "classifier = TextClassifier(CONFIG['input_dim'], CONFIG['hidden_dim'], CONFIG['output_dim'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:08.368976300Z",
     "start_time": "2024-10-12T15:00:07.879556800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция подготовки данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def prepare_data(train_annotations):\n",
    "    audio_files, texts, labels = [], [], []\n",
    "    for annotation in train_annotations:\n",
    "        audio_files.append(annotation['audio_filepath'])\n",
    "        texts.append(annotation['text'])\n",
    "        labels.append(annotation['label'])\n",
    "    return audio_files, texts, labels\n",
    "\n",
    "audio_files, texts, labels = prepare_data(train_annotations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:08.370976700Z",
     "start_time": "2024-10-12T15:00:07.903590200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция для трансформации текста"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(texts, input_dim):\n",
    "    tokenizer = TfidfVectorizer(max_features=input_dim)\n",
    "    return tokenizer.fit_transform(texts).toarray(), tokenizer\n",
    "\n",
    "train_vectors, tokenizer = tokenize_and_vectorize(texts, CONFIG['input_dim'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:08.397606600Z",
     "start_time": "2024-10-12T15:00:07.953318300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Определение датасета и загрузчика данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TextDataset(train_vectors, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:08.397606600Z",
     "start_time": "2024-10-12T15:00:07.973351300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучение модели"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1/10, Потеря: 2123.5666\n",
      "Эпоха 2/10, Потеря: 2119.0384\n",
      "Эпоха 3/10, Потеря: 2118.8926\n",
      "Эпоха 4/10, Потеря: 2118.7522\n",
      "Эпоха 5/10, Потеря: 2117.7375\n",
      "Эпоха 6/10, Потеря: 2116.2456\n",
      "Эпоха 7/10, Потеря: 2117.9703\n",
      "Эпоха 8/10, Потеря: 2115.6429\n",
      "Эпоха 9/10, Потеря: 2113.7787\n",
      "Эпоха 10/10, Потеря: 2114.7357\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, data_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for texts_batch, labels_batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts_batch.float())\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Эпоха {epoch+1}/{num_epochs}, Потеря: {epoch_loss:.4f}\")\n",
    "\n",
    "# Настройки обучения\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Запуск обучения\n",
    "train_model(classifier, train_loader, criterion, optimizer, CONFIG['num_epochs'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:00:57.142488400Z",
     "start_time": "2024-10-12T15:00:37.231934Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Функция для обработки аудио с помощью Vosk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file, model_dir):\n",
    "    wf = wave.open(audio_file, \"rb\")\n",
    "    model = Model(model_dir)\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "\n",
    "    result_text = \"\"\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            result = json.loads(rec.Result())\n",
    "            result_text += result.get(\"text\", \"\")\n",
    "\n",
    "    final_result = json.loads(rec.FinalResult())\n",
    "    result_text += final_result.get(\"text\", \"\")\n",
    "\n",
    "    return result_text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:01:01.481512300Z",
     "start_time": "2024-10-12T15:01:01.432763500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обработка и сохранение результатов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '6ca54494-76ff-11ee-8f2f-c09bf4619c03_1.wav'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 18\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Передайте просто список путей к аудиофайлам\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m transcription_results \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_files\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCONFIG\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmodel_path\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[37], line 4\u001B[0m, in \u001B[0;36mprocess_files\u001B[1;34m(file_list, classifier, tokenizer, model_dir)\u001B[0m\n\u001B[0;32m      2\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m audio_file \u001B[38;5;129;01min\u001B[39;00m file_list:\n\u001B[1;32m----> 4\u001B[0m     transcribed_text \u001B[38;5;241m=\u001B[39m \u001B[43mtranscribe_audio\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     text_vector \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mtransform([transcribed_text])\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[0;32m      6\u001B[0m     text_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(text_vector, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n",
      "Cell \u001B[1;32mIn[34], line 2\u001B[0m, in \u001B[0;36mtranscribe_audio\u001B[1;34m(audio_file, model_dir)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranscribe_audio\u001B[39m(audio_file, model_dir):\n\u001B[1;32m----> 2\u001B[0m     wf \u001B[38;5;241m=\u001B[39m \u001B[43mwave\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     model \u001B[38;5;241m=\u001B[39m Model(model_dir)\n\u001B[0;32m      4\u001B[0m     rec \u001B[38;5;241m=\u001B[39m KaldiRecognizer(model, wf\u001B[38;5;241m.\u001B[39mgetframerate())\n",
      "File \u001B[1;32mC:\\COMPINT\\Python31011\\lib\\wave.py:509\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(f, mode)\u001B[0m\n\u001B[0;32m    507\u001B[0m         mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mWave_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Wave_write(f)\n",
      "File \u001B[1;32mC:\\COMPINT\\Python31011\\lib\\wave.py:159\u001B[0m, in \u001B[0;36mWave_read.__init__\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_i_opened_the_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 159\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_i_opened_the_file \u001B[38;5;241m=\u001B[39m f\n\u001B[0;32m    161\u001B[0m \u001B[38;5;66;03m# else, assume it is an open file object already\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '6ca54494-76ff-11ee-8f2f-c09bf4619c03_1.wav'"
     ]
    }
   ],
   "source": [
    "def process_files(file_list, classifier, tokenizer, model_dir):\n",
    "    results = []\n",
    "    for audio_file in file_list:\n",
    "        transcribed_text = transcribe_audio(audio_file, model_dir)\n",
    "        text_vector = tokenizer.transform([transcribed_text]).toarray()\n",
    "        text_tensor = torch.tensor(text_vector, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            outputs = classifier(text_tensor)\n",
    "        _, predicted_class = torch.max(outputs, 1)\n",
    "        results.append({\n",
    "            \"file_name\": audio_file,\n",
    "            \"transcription\": transcribed_text,\n",
    "            \"category\": predicted_class.item()\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Передайте просто список путей к аудиофайлам\n",
    "transcription_results = process_files(audio_files, classifier, tokenizer, CONFIG['model_path'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T15:06:16.741881500Z",
     "start_time": "2024-10-12T15:06:16.573761100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сохранение результатов в файл"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"transcriptions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transcription_results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Вывод результатов\n",
    "print(json.dumps(transcription_results, indent=4, ensure_ascii=False))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
